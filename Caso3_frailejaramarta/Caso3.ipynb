{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f2d9412-01ea-4a3a-9073-a953620802e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Explorando tendencias de mercado de Airbnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cd0308a-ebab-4a7b-928b-4d11ee759001",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import (\n",
    "    SparkSession,\n",
    "    types,\n",
    "    functions as F,\n",
    ")\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('airbnb')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e16c361b-3451-4c88-9c67-22c5029d67a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cargar archivos de diferentes formatos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ed112f6-7f8f-45b7-b679-16d991b1d3ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cargar archivo TSV, CSV, Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b604dc29-a8ae-443d-85d4-080107913a65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+---------------+\n|listing_id|       host_name|    last_review|\n+----------+----------------+---------------+\n|      2595|        Jennifer|    May 21 2019|\n|      3831|     LisaRoxanne|   July 05 2019|\n|      5099|           Chris|   June 22 2019|\n|      5178|        Shunichi|   June 24 2019|\n|      5238|             Ben|   June 09 2019|\n|      5295|            Lena|   June 22 2019|\n|      5441|            Kate|   June 23 2019|\n|      5803|          Laurie|   June 24 2019|\n|      6021|         Claudio|   July 05 2019|\n|      6848|   Allen & Irina|   June 29 2019|\n|      7097|            Jane|   June 28 2019|\n|      7322|            Doti|   July 01 2019|\n|      7726|Adam And Charity|   June 22 2019|\n|      8024|           Lisel|   July 01 2019|\n|      8025|           Lisel|January 01 2019|\n|      8110|           Lisel|   July 02 2019|\n|      8490|        Nathalie|   June 19 2019|\n|      8505|         Gregory|   June 23 2019|\n|      9518|            Shon|   June 15 2019|\n|      9657|            Dana|  April 19 2019|\n+----------+----------------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_last_review = spark.read.option(\"delimiter\", \"\\t\").csv(\"dbfs:/FileStore/airbnb_last_review.tsv\", header=True, inferSchema=True)\n",
    "df_last_review.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcaefac4-e5a9-43b5-b313-0b0e383d2150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+\n|listing_id|      price|         nbhood_full|\n+----------+-----------+--------------------+\n|      2595|225 dollars|  Manhattan, Midtown|\n|      3831| 89 dollars|Brooklyn, Clinton...|\n|      5099|200 dollars|Manhattan, Murray...|\n|      5178| 79 dollars|Manhattan, Hell's...|\n|      5238|150 dollars|Manhattan, Chinatown|\n|      5295|135 dollars|Manhattan, Upper ...|\n|      5441| 85 dollars|Manhattan, Hell's...|\n|      5803| 89 dollars|Brooklyn, South S...|\n|      6021| 85 dollars|Manhattan, Upper ...|\n|      6848|140 dollars|Brooklyn, William...|\n|      7097|215 dollars|Brooklyn, Fort Gr...|\n|      7322|140 dollars|  Manhattan, Chelsea|\n|      7726| 99 dollars|Brooklyn, Crown H...|\n|      8024|130 dollars|Brooklyn, Park Slope|\n|      8025| 80 dollars|Brooklyn, Park Slope|\n|      8110|110 dollars|Brooklyn, Park Slope|\n|      8490|120 dollars|Brooklyn, Bedford...|\n|      8505| 60 dollars|Brooklyn, Windsor...|\n|      9518| 44 dollars|   Manhattan, Inwood|\n|      9657|180 dollars|Manhattan, East V...|\n+----------+-----------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_price = spark.read.csv(\"dbfs:/FileStore/airbnb_price.csv\", header=True, inferSchema=True)\n",
    "df_price.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfd05a6d-6c6d-49c1-b1bb-39a5d7ae57bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------------+\n|listing_id|         description|      room_type|\n+----------+--------------------+---------------+\n|      2595|Skylit Midtown Ca...|Entire home/apt|\n|      3831|Cozy Entire Floor...|Entire home/apt|\n|      5099|Large Cozy 1 BR A...|Entire home/apt|\n|      5178|Large Furnished R...|   private room|\n|      5238|Cute & Cozy Lower...|Entire home/apt|\n|      5295|Beautiful 1br on ...|Entire home/apt|\n|      5441|Central Manhattan...|   Private room|\n|      5803|Lovely Room 1, Ga...|   Private room|\n|      6021|Wonderful Guest B...|   Private room|\n|      6848|Only 2 stops to M...|entire home/apt|\n|      7097|Perfect for Your ...|Entire home/apt|\n|      7322|     Chelsea Perfect|   Private room|\n|      7726|Hip Historic Brow...|Entire home/apt|\n|      8024|CBG CtyBGd HelpsH...|   PRIVATE ROOM|\n|      8025|CBG Helps Haiti R...|   PRIVATE ROOM|\n|      8110|CBG Helps Haiti R...|   Private room|\n|      8490|MAISON DES SIRENE...|Entire home/apt|\n|      8505|Sunny Bedroom Acr...|   Private room|\n|      9518|SPACIOUS, LOVELY ...|   PRIVATE ROOM|\n|      9657|Modern 1 BR / NYC...|entire home/apt|\n+----------+--------------------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Leer el archivo CSV con comillas\n",
    "df_room_type_raw = spark.read.csv(\"/FileStore/airbnb_room_type.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Limpiar las comillas de las columnas usando `withColumn` y `regexp_replace`\n",
    "df_room_type = df_room_type_raw.select(\n",
    "    [F.regexp_replace(F.col(col), '\"', '').alias(col) for col in df_room_type_raw.columns]\n",
    ")\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame\n",
    "df_room_type.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fba7dd94-0866-44fa-a640-fabdca2d9300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ¿Cuáles son las fechas de las primeras y las últimas reseñas? Almacene estos valores como dos variables independientes con los nombres que prefiera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86d0b80c-1609-42b1-b46e-86f58b83bbea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Lo primero que he hecho ha sido formatear la fecha, y como no me estaba funcionando nada puees lo he hehco de una manera un poco mas manual con un minidiccionario para el mes y luego remplazanod los espacios por las barras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "548254f8-d84d-4ad3-b77a-58eb16375962",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-----------+\n|listing_id|host_name       |last_review|\n+----------+----------------+-----------+\n|2595      |Jennifer        |05-21-2019 |\n|3831      |LisaRoxanne     |07-05-2019 |\n|5099      |Chris           |06-22-2019 |\n|5178      |Shunichi        |06-24-2019 |\n|5238      |Ben             |06-09-2019 |\n|5295      |Lena            |06-22-2019 |\n|5441      |Kate            |06-23-2019 |\n|5803      |Laurie          |06-24-2019 |\n|6021      |Claudio         |07-05-2019 |\n|6848      |Allen & Irina   |06-29-2019 |\n|7097      |Jane            |06-28-2019 |\n|7322      |Doti            |07-01-2019 |\n|7726      |Adam And Charity|06-22-2019 |\n|8024      |Lisel           |07-01-2019 |\n|8025      |Lisel           |01-01-2019 |\n|8110      |Lisel           |07-02-2019 |\n|8490      |Nathalie        |06-19-2019 |\n|8505      |Gregory         |06-23-2019 |\n|9518      |Shon            |06-15-2019 |\n|9657      |Dana            |04-19-2019 |\n+----------+----------------+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace\n",
    "\n",
    "# Crear un diccionario para reemplazar los nombres de los meses por números\n",
    "month_dict = {\n",
    "    \"January\": \"01\", \"February\": \"02\", \"March\": \"03\", \"April\": \"04\", \"May\": \"05\", \"June\": \"06\",\n",
    "    \"July\": \"07\", \"August\": \"08\", \"September\": \"09\", \"October\": \"10\", \"November\": \"11\", \"December\": \"12\"\n",
    "}\n",
    "\n",
    "# Reemplazar los nombres de los meses en la columna 'last_review' por el número de mes correspondiente\n",
    "for month, num in month_dict.items():\n",
    "    df_last_review = df_last_review.withColumn(\"last_review\", regexp_replace(\"last_review\", month, num))\n",
    "\n",
    "# Reemplazar el espacio entre los componentes de la fecha por un guion\n",
    "df_last_review = df_last_review.withColumn(\"last_review\", regexp_replace(\"last_review\", \" \", \"-\"))\n",
    "\n",
    "# Mostrar el resultado final\n",
    "df_last_review.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bf55ea9-b857-425a-8fcf-1641c6f195ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Luego ya como tienen el formato aplicado solo tenemos que hacer un minimo y un maximo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f47a0289-984e-4e57-892b-10a01142c473",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecha de la primera reseña: 01-01-2019\nFecha de la última reseña: 07-09-2019\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "# Obtener la fecha de la primera reseña (mínima) y la última reseña (máxima)\n",
    "first_review_date = df_last_review.select(min(\"last_review\")).collect()[0][0]\n",
    "last_review_date = df_last_review.select(max(\"last_review\")).collect()[0][0]\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(f\"Fecha de la primera reseña: {first_review_date}\")\n",
    "print(f\"Fecha de la última reseña: {last_review_date}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b21c4012-e33e-48a2-a8d2-b6da22821643",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ¿Cuántos de los anuncios son habitaciones privadas? Guárdelo en cualquier variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89f42f20-198c-4436-a8ef-31282c5258a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> Aqui tambien hay un problema y esq el termino de \"private room\" cada vez esta escrito de una manera, entonces lo que pasa es que hay que convertirlo todo a minusculas y luego ya buscar eso en la columna de `room_type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b4f646f-1989-42b2-9fbf-fbe1a58d58d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de anuncios de habitaciones privadas: 1942\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Convertir la columna 'room_type' a minúsculas\n",
    "df_room_type_clean = df_room_type.withColumn(\"room_type_lower\", F.lower(F.col(\"room_type\")))\n",
    "\n",
    "# Filtrar los anuncios que son habitaciones privadas\n",
    "df_private_rooms = df_room_type_clean.filter(F.col(\"room_type_lower\").contains(\"private room\"))\n",
    "\n",
    "# Contar el número de anuncios de habitaciones privadas\n",
    "num_private_rooms = df_private_rooms.count()\n",
    "\n",
    "# Guardar el resultado en una variable\n",
    "num_private_rooms\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(f\"Número de anuncios de habitaciones privadas: {num_private_rooms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34a6904d-b1b5-43fb-b68a-29d15b1bba40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ¿Cuál es el precio promedio de los anuncios? Redondee a los dos decimales más cercanos y guárdelo en una variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c794d284-c09d-408b-bfd4-8d2850f54833",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> Aqui nos encontramos el problema de que la tabla no es numerica sino que tiene puesto \"dollars\" en todos los campos, asiq si no queremos que salga nulo hay que limpiarlos tambien antes eliminando esa palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c4b486d-2e65-4baf-ab7a-a59f45984c40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El precio promedio de los anuncios es: 141.78\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Limpiar la columna 'price' eliminando la palabra 'dollars' y cualquier otro texto\n",
    "df_price_clean = df_price.withColumn(\"price\", F.regexp_replace(F.col(\"price\"), \" dollars\", \"\").cast(\"double\"))\n",
    "\n",
    "# Eliminar filas con valores nulos en la columna 'price'\n",
    "df_price_clean = df_price_clean.filter(F.col(\"price\").isNotNull())\n",
    "\n",
    "# Calcular el precio promedio de los anuncios\n",
    "average_price = df_price_clean.agg(F.avg(\"price\").alias(\"average_price\")).collect()[0][\"average_price\"]\n",
    "\n",
    "# Verificar si el resultado es None\n",
    "if average_price is not None:\n",
    "    average_price_rounded = round(average_price, 2)\n",
    "else:\n",
    "    average_price_rounded = None\n",
    "\n",
    "# Imprimir el resultado\n",
    "print(f\"El precio promedio de los anuncios es: {average_price_rounded}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "486c1428-2ed0-43fc-8443-287cedeb3e34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Combine las nuevas variables en un DataFrame llamado review_dates con cuatro columnas en el siguiente orden: first_reviewed, last_reviewed, nb_private_rooms y avg_price. El DataFrame solo debe contener una fila de valores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49961693-2f97-4024-b5c0-714c26abc848",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Aqui yo entiendo que las nuevas variables de las que me habla son: \n",
    "- first_review_date \n",
    "- last_review_date\n",
    "- num_private_rooms\n",
    "- average_price_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8907553a-f791-4817-a163-3046bfb6317d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+----------------+---------+\n|first_reviewed|last_reviewed|nb_private_rooms|avg_price|\n+--------------+-------------+----------------+---------+\n|    01-01-2019|   07-09-2019|            1942|   141.78|\n+--------------+-------------+----------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Crear un Row con los valores de las variables\n",
    "review_dates_row = Row(\n",
    "    first_reviewed=first_review_date,\n",
    "    last_reviewed=last_review_date,\n",
    "    nb_private_rooms=num_private_rooms,\n",
    "    avg_price=average_price_rounded\n",
    ")\n",
    "\n",
    "# Crear un DataFrame con esta fila\n",
    "review_dates = spark.createDataFrame([review_dates_row])\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "review_dates.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1312f19-7a9b-4cd6-a50e-5354dae22db1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Caso3",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
